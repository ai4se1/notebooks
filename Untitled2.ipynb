{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfa9fd0-9e27-4b12-8518-def7aa0efff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f066e7-880d-4d58-959c-eba49a60efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19397556-4941-4b40-b100-04a9f6c50ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffa24b1-7be9-43d0-9171-c0809c314daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/codegemma-2b\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2c35ba-c51d-4608-968b-1ae854da2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Winter', 'kn', 'ub', 'bel', 'baum']\n",
      "['Sch', 'Ã¤', 'fer']\n",
      "['R', 'enn', 'rad']\n",
      "['ðŸ˜Š']\n",
      "['ðŸ«‚']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('Winterknubbelbaum'))\n",
    "print(tokenizer.tokenize(\"SchÃ¤fer\"))\n",
    "print(tokenizer.tokenize(\"Rennrad\"))\n",
    "print(tokenizer.tokenize(\"ðŸ˜Š\"))\n",
    "print(tokenizer.tokenize(\"ðŸ«‚\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "317edb95-c655-404c-85ca-b77094bfb95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d05a9b4e3a46c48d2d2824b08b55fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "105bcaa0-8519-4c14-bd45-f57c5f5246f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''def hello_world.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "302f1452-e1fe-4ca4-a7d9-78c9f3475b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52830e50-0ad9-4bd7-abd1-5d15b65cca01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Winter', 'kn', 'ub', 'bel', 'baum']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Winterknubbelbaum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d97f13b-5eea-4ddf-b197-5d0bd8e16b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000\n",
      "tensor([2.8011e-11, 7.9222e-07, 8.9770e-07,  ..., 2.2032e-10, 4.9651e-10,\n",
      "        3.1741e-11], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2158, device='cuda:0')\n",
      "['py']\n",
      "beam--------------------------------\n",
      "<bos>def hello_world.py\n",
      "<|fim_prefix|><|fim_suffix|><|fim_middle|>def hello_world():\n",
      "    print(\"hello world\")\n",
      "\n",
      "hello_world()<|file_separator|><eos>\n",
      "top-p-------------------------\n",
      "<bos>def hello_world.py\n",
      "<|fim_prefix|><|fim_suffix|><|fim_middle|>def hello_world():\n",
      "    print(\"hello world\")\n",
      "\n",
      "def sum():\n",
      "    print(1+2)<|file_separator|><eos>\n",
      "top-k--------------------------\n",
      "<bos>def hello_world.c\n",
      "\n",
      "#include \"hello_world\"\n",
      "\n",
      "\n",
      "int main()\n",
      "{\n",
      "    printf(\"Hello world\\n\");\n",
      "    return 0;\n",
      "<|file_separator|>hello_world/src/main.h\n",
      "<|fim_prefix|>\n",
      "void hello<|fim_suffix|><|fim_middle|>_world();<|file_separator|>def hello_world.c\n",
      "#include <stdio.h>\n",
      "\n",
      "void hello_wold() {\n",
      "  printf(\"Hello world\");\n",
      "  \n",
      "}<|file_separator|><eos>\n"
     ]
    }
   ],
   "source": [
    "prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "res = model(**inputs).logits[0][-1]\n",
    "print(len(res))\n",
    "\n",
    "res = torch.softmax(res, 0)\n",
    "print(res)\n",
    "\n",
    "res = torch.argmax(res)\n",
    "\n",
    "print(res)\n",
    "\n",
    "ninput = tokenizer.convert_ids_to_tokens([res])\n",
    "print(ninput)\n",
    "\n",
    "print(\"beam--------------------------------\")\n",
    "outputs = model.generate(**inputs, num_beams=8, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "print(\"top-p-------------------------\")\n",
    "outputs = model.generate(**inputs, do_sample=True, top_p=0.9, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "print(\"top-k--------------------------\")\n",
    "outputs = model.generate(**inputs, do_sample=True, top_k=3, max_new_tokens=256, temperature=2.0)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "#outputs = model.generate(**inputs, max_new_tokens=4000)\n",
    "#print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b3f8cf-7651-4ced-8c84-d973d7f1a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a814ecaa-ae0f-4c61-9325-fab5270c6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into bare repository 'flask.git'...\n",
      "remote: Enumerating objects: 24783, done.\u001b[K\n",
      "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 24783 (delta 50), reused 100 (delta 33), pack-reused 24652\u001b[K\n",
      "Receiving objects: 100% (24783/24783), 10.19 MiB | 15.83 MiB/s, done.\n",
      "Resolving deltas: 100% (16595/16595), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --bare https://github.com/pallets/flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd9d759-8387-461d-8fff-1ab30a4ffc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('./flask.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e8b8c1-e019-44e8-9887-6040baba7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = repo.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfb674a-7295-499f-b50d-a2de00e2d06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11c15ddfeb6edcb0978d3407ed972ae441013177'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.commit.hexsha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd00a08-2ed1-4a7b-ad02-7b7a8e45848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HEAD'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa0eb84-6291-4fc9-9205-eda3df588f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.cmd.Git at 0x7f87846fabc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b459c3-049d-44eb-be9f-112d0afdd43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/flask.git'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.git_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1c81b-7380-4647-bfb8-26e930355c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
