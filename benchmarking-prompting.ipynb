{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import difflib\n",
    "import autopep8\n",
    "import requests as req\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import json\n",
    "import tree_sitter_python as tspython\n",
    "import tree_sitter_java as tsjava\n",
    "\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset loading and preprocessing\n",
    "\n",
    "We only use logic errors and perform the following preprocessing steps:\n",
    "- Remove examples containing syntax errors\n",
    "- Remove blank lines\n",
    "- Format the buggy code and the solution\n",
    "- Remove comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Rtian/DebugBench\")\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "filtered_df = df[df['category'] == 'logic error'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PY_LANGUAGE = Language(tspython.language(), \"python\")\n",
    "JAVA_LANGUAGE = Language(tsjava.language(), \"java\")\n",
    "\n",
    "py_parser = Parser()\n",
    "py_parser.set_language(PY_LANGUAGE)\n",
    "\n",
    "java_parser = Parser()\n",
    "java_parser.set_language(JAVA_LANGUAGE)\n",
    "\n",
    "def remove_blank_lines(text):\n",
    "    return \"\\n\".join([s.rstrip() for s in text.splitlines() if s.strip()])\n",
    "    \n",
    "def format_cpp_code(code):\n",
    "    try:\n",
    "        process = subprocess.Popen(['clang-format', '--style=file:./clang-format.txt'], \n",
    "                                   stdin=subprocess.PIPE, \n",
    "                                   stdout=subprocess.PIPE, \n",
    "                                   stderr=subprocess.PIPE)\n",
    "        formatted_code, errors = process.communicate(input=code.encode())\n",
    "        if process.returncode != 0:\n",
    "            print(\"Error cpp formatting code: \", errors.decode())\n",
    "            return code\n",
    "        process = subprocess.Popen(['g++', '-fpreprocessed', '-dD', '-E', '-P', '-'],\n",
    "                                   stdin=subprocess.PIPE,\n",
    "                                   stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE)\n",
    "        formatted_code, errors = process.communicate(input=formatted_code)\n",
    "\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(\"Error cpp formatting code: \", errors.decode())\n",
    "            return code\n",
    "        \n",
    "        return remove_blank_lines(formatted_code.decode())\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "        return code\n",
    "    \n",
    "def format_python_code(code):\n",
    "    try:\n",
    "        # ignore errors to prevent autopep from running forever\n",
    "        formatted_code = autopep8.fix_code(code, options={'ignore': ['E']})\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "        return code\n",
    "    bytes_code = bytes(formatted_code, \"utf-8\")\n",
    "    array = bytearray(bytes_code)\n",
    "    tree = py_parser.parse(bytes_code)\n",
    "    def traverse(node):\n",
    "        if node.type == 'comment':\n",
    "            array[node.start_byte:node.end_byte]=(node.end_byte - node.start_byte) * b\" \"\n",
    "            return \n",
    "        elif node.child_count == 0:\n",
    "            return\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "            return\n",
    "    traverse(tree.root_node)\n",
    "    cleaned_code = array.decode(\"utf-8\")\n",
    "    return remove_blank_lines(cleaned_code)\n",
    "\n",
    "\n",
    "def format_java_code(code):\n",
    "    try:\n",
    "        process = subprocess.Popen(['clang-format', '--style=file:./clang-format-java.txt', '--assume-filename=Main.java'], \n",
    "                                   stdin=subprocess.PIPE, \n",
    "                                   stdout=subprocess.PIPE, \n",
    "                                   stderr=subprocess.PIPE)\n",
    "        formatted_code, errors = process.communicate(input=code.encode())\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(\"Error formatting java code: \", errors.decode())\n",
    "            return code\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "        return code\n",
    "    \n",
    "    array = bytearray(formatted_code)\n",
    "    tree = java_parser.parse(formatted_code)\n",
    "    def traverse(node):\n",
    "        if \"comment\" in node.type:\n",
    "            #print(f\"Removed comment: {array[node.start_byte:node.end_byte].decode('utf-8')}\")\n",
    "            array[node.start_byte:node.end_byte]=(node.end_byte - node.start_byte) * b\" \"\n",
    "            return \n",
    "        elif node.child_count == 0:\n",
    "            return\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "            return\n",
    "    \n",
    "    traverse(tree.root_node)\n",
    "    cleaned_code = array.decode(\"utf-8\")\n",
    "    return remove_blank_lines(cleaned_code)\n",
    "\n",
    "def format_code(code, language):\n",
    "    if language == \"python3\":\n",
    "        return format_python_code(code)\n",
    "    elif language == \"cpp\":\n",
    "        return format_cpp_code(code)\n",
    "    elif language == \"java\":\n",
    "        return format_java_code(code)\n",
    "    else:\n",
    "        print(f\"Unsupported language: {language}\")\n",
    "        return code\n",
    "\n",
    "def format_entry(entry):\n",
    "    buggy_code = entry['buggy_code']\n",
    "    solution = entry['solution']\n",
    "    return pd.Series({\n",
    "        \"buggy_code_formatted\": format_code(buggy_code, entry['language']),\n",
    "        \"solution_formatted\": format_code(solution, entry['language'])\n",
    "    })\n",
    "    \n",
    "filtered_df[[\"buggy_code_formatted\", \"solution_formatted\"]] = filtered_df.progress_apply(format_entry, axis=1)\n",
    "\n",
    "formatted_df = filtered_df\n",
    "formatted_df.to_csv(\"formatted_code.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the LLM\n",
    "After the preprocessing, we now retrieve lines of code that are relevant for fixing the buggy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = pd.read_csv(\"./notebooks/formatted_code.csv\")\n",
    "\n",
    "\n",
    "formatted_df['buggy_code'] = formatted_df['buggy_code'].apply(str)\n",
    "formatted_df['buggy_code_formatted'] = formatted_df['buggy_code_formatted'].apply(str)\n",
    "\n",
    "formatted_df['solution'] = formatted_df['solution'].apply(str)\n",
    "formatted_df['solution_formatted'] = formatted_df['solution_formatted'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_changed_lines(file1_contents, file2_contents, print_diff=False):\n",
    "    diff = difflib.ndiff(file1_contents.splitlines(), file2_contents.splitlines())\n",
    "    \n",
    "    changed_lines = set() # change or insert below\n",
    "\n",
    "    current_line_number = 0\n",
    "    pending_change = False\n",
    "    for line in diff: \n",
    "        if print_diff:\n",
    "            print(f\"{current_line_number}: {line}\")\n",
    " \n",
    "        if line.startswith('  '):\n",
    "            current_line_number += 1\n",
    "            pending_change = False\n",
    "        else:\n",
    "            if line.startswith('- '):  # Lines in file1 but not in file2\n",
    "                pending_change = True\n",
    "                current_line_number += 1\n",
    "                changed_lines.add(current_line_number)\n",
    "            elif line.startswith('+ '):  # Lines in file2 but not in file1\n",
    "                if pending_change:\n",
    "                    pending_change = False\n",
    "                else:\n",
    "                    changed_lines.add(current_line_number)\n",
    "    return list(changed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = \"output_with_context2.json\"\n",
    "results_file_name = \"results_formatted_context.csv\"\n",
    "\n",
    "log_file = open(log_file_name, \"w\")\n",
    "log_file.write(\"[\")\n",
    "\n",
    "def retrieve_relevant_lines_for_entry(entry):\n",
    "    buggy_code = entry['buggy_code_formatted']\n",
    "    solution = entry['solution_formatted']\n",
    "    num_lines = buggy_code.count('\\n')\n",
    "    changed_lines = find_changed_lines(buggy_code, solution)\n",
    "\n",
    "    data = {\"code\": buggy_code, \"language\": entry['language'], \"context\": entry[\"question\"]}\n",
    "    success = True\n",
    "    response_json = []\n",
    "    try:\n",
    "        resp = req.post(\"http://delos.eaalab.hpi.uni-potsdam.de:8010/highlight-code/\", json=data)\n",
    "        try:\n",
    "            response_json = resp.json()\n",
    "        except:\n",
    "            print(\"Json parsing failed\")\n",
    "            print(resp.text())\n",
    "    except:\n",
    "        print(\"Request failed\")\n",
    "        success = False    \n",
    "\n",
    "    predicted_lines = []\n",
    "    suggestions = []\n",
    "    descriptions = []\n",
    "    actions = []\n",
    "    for item in response_json:\n",
    "        line_number = int(item.get('line_number', \"1\"))\n",
    "        suggestions.append(item.get(\"description\", \"\"))\n",
    "        descriptions.append(item.get(\"suggestion\", \"\"))\n",
    "        predicted_lines.append(line_number)\n",
    "        actions.append(item.get(\"action\", \"\"))\n",
    "\n",
    "    result = {\n",
    "        'buggy_code': buggy_code,\n",
    "        'solution': solution,\n",
    "        'changed_lines': changed_lines,\n",
    "        'predicted_lines': predicted_lines,\n",
    "        'num_lines': num_lines,\n",
    "        'success': success,\n",
    "        'suggestions': suggestions,\n",
    "        'descriptions': descriptions,\n",
    "        'actions': actions,\n",
    "    }\n",
    "    log_file.write(f'{json.dumps(result)},')\n",
    "    log_file.flush()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = formatted_df.iloc[318:]\n",
    "print(filtered_df.head())\n",
    "results = filtered_df.progress_apply(retrieve_relevant_lines_for_entry, axis=1)\n",
    "\n",
    "log_file.write(\"]\")\n",
    "log_file.close()\n",
    "results_df = pd.DataFrame(results.tolist())\n",
    "results_df.to_csv(results_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True\n",
    "if not skip:\n",
    "    import ast\n",
    "    df = pd.read_csv(\"./notebooks/results_formatted.csv\")\n",
    "    #df['num_clusters'] = df['changed_lines'].apply(ast.literal_eval[1])\n",
    "    df['changed_lines'] = df['changed_lines'].apply(ast.literal_eval)\n",
    "    df['predicted_lines'] = df['predicted_lines'].apply(ast.literal_eval)\n",
    "    df['actions'] = df['actions'].apply(ast.literal_eval)\n",
    "    # tell pythont hat buggy_code is a string\n",
    "    df['buggy_code'] = df['buggy_code'].apply(str)\n",
    "    df['solution'] = df['solution'].apply(str)\n",
    "else:\n",
    "    df = results_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove changed lines and num_clusters from the results\n",
    "#df = df.drop(columns=['changed_lines', 'num_clusters'])\n",
    "## recompute changed lines\n",
    "#df['changed_lines'] = df.apply(lambda x: find_changed_lines(x['buggy_code'], x['solution']), axis=1)\n",
    "#df.to_csv(\"results_formatted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use values to ignore underlying indexes\n",
    "df['language'] = filtered_df['language'].values\n",
    "df['level'] = filtered_df['level'].values\n",
    "df['category'] = filtered_df['category'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_df = df[df[\"success\"]]\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "# We need the following line when working with success_df instead of df\n",
    "#pd.options.mode.copy_on_write = False\n",
    "print(f'Number of successful requests: {len(success_df)}')\n",
    "print(f'Number of unsuccessful requests: {len(df) - len(success_df)}')\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = success_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(row):\n",
    "    labels = pd.Series([0] * (row['num_lines'] + 1)) # Why +1? \n",
    "    for line in row['changed_lines']:\n",
    "        if line > row['num_lines'] +1:\n",
    "            print(f\"Error {line} exceeds {row['num_lines']}\")\n",
    "            continue\n",
    "        if line == 0: # Special case in which an insertion before the first line is required.\n",
    "            labels[0] = 1\n",
    "        else: \n",
    "            labels[line-1] = 1 # lines are 1-indexed so we subtract 1\n",
    "    assert len(labels) == row['num_lines']+1, f\"{len(labels)} != {row['num_lines']}\"\n",
    "    return labels.tolist()\n",
    "\n",
    "def create_predictions(row):\n",
    "    predictions = pd.Series([0] * (row['num_lines'] +1))\n",
    "    for i, line in enumerate(row['predicted_lines']):\n",
    "        action = row['actions'][i]\n",
    "        relevant_line = line\n",
    "        if action == \"insert_below\":\n",
    "            pass\n",
    "        elif action == \"insert_above\":\n",
    "            relevant_line -= 1\n",
    "        elif action == \"change\" or action == \"delete\" or action == \"remove\" or action == \"modify\":\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Unknown action: {action}\")\n",
    "            continue\n",
    "\n",
    "        if relevant_line > row['num_lines'] + 1:\n",
    "            print(f\"Error {line} exceeds {row['num_lines']}\")\n",
    "            continue\n",
    "        predictions[relevant_line-1] = 1\n",
    "    assert len(predictions) == row['num_lines'] +1, f\"{len(labels)} != {row['num_lines']}\"\n",
    "    return predictions.tolist()\n",
    "\n",
    "eval_df['labels'] = eval_df.apply(create_labels, axis=1)\n",
    "eval_df['predictions'] = eval_df.apply(create_predictions, axis=1)\n",
    "\n",
    "labels = np.array([b for a in eval_df['labels'].values for b in a])\n",
    "predictions = np.array([b for a in eval_df['predictions'].values for b in a])\n",
    "assert len(labels) == len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def entry_based_metrics(entry):\n",
    "    labels = np.array(entry['labels'])\n",
    "    predictions = np.array(entry['predictions'])\n",
    "    conf_matrix = confusion_matrix(labels, predictions, labels=[0, 1])\n",
    "\n",
    "    return {'tn': conf_matrix[0][0],\n",
    "            'fn': conf_matrix[1][0],\n",
    "            'fp': conf_matrix[0][1],\n",
    "            'tp': conf_matrix[1][1],\n",
    "            'f1': f1_score(labels, predictions)}\n",
    "\n",
    "\n",
    "\n",
    "results = eval_df.progress_apply(entry_based_metrics, axis=1)\n",
    "\n",
    "results_df = pd.DataFrame(results.tolist())\n",
    "\n",
    "eval_df['tp'] = results_df['tp']\n",
    "eval_df['fp'] = results_df['fp']\n",
    "eval_df['fn'] = results_df['fn']\n",
    "eval_df['tn'] = results_df['tn']\n",
    "eval_df['f1'] = results_df['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(eval_df['f1'], bins=200, edgecolor='black')\n",
    "plt.title('Histogram of F1 Scores')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "res_df = eval_df.sort_values(by='f1', ascending=False).head(2)#[['buggy_code', 'solution', 'changed_lines', 'predicted_lines']]\n",
    "\n",
    "def pretty_print(input_df):\n",
    "    return display(HTML(input_df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "pretty_print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_print(input_df):\n",
    "    tn = input_df['tn'].sum()\n",
    "    fn = input_df['fn'].sum()\n",
    "    tp = input_df['tp'].sum()\n",
    "    fp = input_df['fp'].sum()\n",
    "    print([tn, fp], \"\\n\",\n",
    "           [fn, tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"differen levels\")\n",
    "easy_df = eval_df[eval_df['level'] == 'easy']\n",
    "aggregate_and_print(easy_df)\n",
    "mid_df = eval_df[eval_df['level'] == 'medium']\n",
    "aggregate_and_print(mid_df)\n",
    "hard_df = eval_df[eval_df['level'] == 'hard']\n",
    "aggregate_and_print(hard_df)\n",
    "\n",
    "print(\"non zero\")\n",
    "non_zero_df = eval_df[eval_df['suggestions'].apply(lambda x: len(x) > 0)]\n",
    "aggregate_and_print(non_zero_df)\n",
    "\n",
    "print(\"zero\")\n",
    "zero_df = eval_df[eval_df['suggestions'].apply(lambda x: len(x) == 0)]\n",
    "aggregate_and_print(zero_df)\n",
    "\n",
    "print(\"error types\")\n",
    "easy_df = eval_df[eval_df['category'] == 'syntax error']\n",
    "aggregate_and_print(easy_df)\n",
    "mid_df = eval_df[eval_df['category'] == 'multiple error']\n",
    "aggregate_and_print(mid_df)\n",
    "hard_df = eval_df[eval_df['category'] == 'logic error']\n",
    "aggregate_and_print(hard_df)\n",
    "ref_df = eval_df[eval_df['category'] == 'reference error']\n",
    "aggregate_and_print(ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "precision = precision_score(labels, predictions)\n",
    "recall = recall_score(labels, predictions)\n",
    "f1 = f1_score(labels, predictions)\n",
    "conf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Balanced Accuracy: {balanced_acc}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "num_positives = labels.sum()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels, predictions)\n",
    "precision, recall, thresholds2 = precision_recall_curve(labels, predictions)\n",
    "fn = [(num_positives) - tp * num_positives for tp in tpr]\n",
    "f1 = [(2*tpr[i]*num_positives)/(2*tpr[i]*num_positives + fpr[i]*num_positives + fn[i]) for i in range(len(fpr))]\n",
    "print(max(f1))\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area\", roc_auc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, fpr, color='blue')\n",
    "plt.plot(thresholds, tpr, color='red')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Classification Thresholds')\n",
    "plt.ylabel('True/False Positive Rate')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, f1, color='navy')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Classification Thresholds')\n",
    "plt.ylabel('F1-Score')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(precision, recall, color='darkorange', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "length = min(len(precision), len(recall))\n",
    "plt.figure()\n",
    "plt.plot(thresholds2, precision[:length-1], color='blue')\n",
    "plt.plot(thresholds2, recall[:length-1], color='red')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Classification Thresholds')\n",
    "plt.ylabel('Precision blue / recall red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
