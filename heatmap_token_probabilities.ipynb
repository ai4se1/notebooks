{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5da06c",
   "metadata": {},
   "source": [
    "# Heatmap of token probabilities\n",
    "\n",
    "This notebook offers a simple script that can highlight \"unexpected\" token within a prompt. The highlighting is based on the difference between predicted token probability and actual token probability.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the prompt 'print(\"Hello}World\")'.\n",
    "After processing 'print(\"Hello',the token '}' would get very low probability of being the next token. That's why the following script would highligt it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00124c14-6427-4382-9ddd-bc1392fc50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8220d04b-0b3f-4f23-98d9-d0aa4c89a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943a3696-f2c3-4229-9e4c-76b0abc50d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu_pytorch_tanh`, edit the `model.config` to set `hidden_activation=gelu_pytorch_tanh`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b109ce778e45598e387c16f4d2e8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/codegemma-1.1-7b-it\"\n",
    "tokenizer = GemmaTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=gpu, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb0b9d6-b88b-4c27-b0df-33b1031acd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_from_hex(r, g, b, text):\n",
    "    return f\"\\x1B[38;2;{r};{g};{b}m{text}\\x1B[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131fa4c5-13f8-45ee-af03-edf7e74ae030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;240;0mtest\u001b[0m \u001b[38;2;255;0;0mtest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(escape_from_hex(255, 240, 0, \"test\"), escape_from_hex(255, 0, 0, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7616d402-72ff-4995-9f9c-737b8090ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens torch.Size([1, 71])\n",
      "\u001b[38;2;0;0;0m<bos>\u001b[0m\u001b[38;2;255;0;0m\n",
      "\u001b[0m\u001b[38;2;77;0;0mYou\u001b[0m\u001b[38;2;0;0;0m are\u001b[0m\u001b[38;2;44;0;0m debugging\u001b[0m\u001b[38;2;213;0;0m▁some\u001b[0m\u001b[38;2;0;0;0m code\u001b[0m\u001b[38;2;74;0;0m.\u001b[0m\u001b[38;2;177;0;0m▁Can\u001b[0m\u001b[38;2;0;0;0m you\u001b[0m\u001b[38;2;36;0;0m help\u001b[0m\u001b[38;2;0;0;0m me\u001b[0m\u001b[38;2;70;0;0m▁identify\u001b[0m\u001b[38;2;0;0;0m the\u001b[0m\u001b[38;2;70;0;0m▁issues\u001b[0m\u001b[38;2;0;0;0m in\u001b[0m\u001b[38;2;86;0;0m▁this\u001b[0m\u001b[38;2;226;0;0m▁piece\u001b[0m\u001b[38;2;0;0;0m of\u001b[0m\u001b[38;2;0;0;0m code\u001b[0m\u001b[38;2;0;0;0m?\u001b[0m\u001b[38;2;250;0;0m▁Can\u001b[0m\u001b[38;2;0;0;0m you\u001b[0m\u001b[38;2;178;0;0m▁give\u001b[0m\u001b[38;2;0;0;0m me\u001b[0m\u001b[38;2;131;0;0m▁for\u001b[0m\u001b[38;2;221;0;0m▁every\u001b[0m\u001b[38;2;241;0;0m▁line\u001b[0m\u001b[38;2;33;0;0m a\u001b[0m\u001b[38;2;94;0;0m▁probability\u001b[0m\u001b[38;2;219;0;0m▁if\u001b[0m\u001b[38;2;164;0;0m▁this\u001b[0m\u001b[38;2;0;0;0m line\u001b[0m\u001b[38;2;79;0;0m▁contains\u001b[0m\u001b[38;2;145;0;0m▁a\u001b[0m\u001b[38;2;0;0;0m bug\u001b[0m\u001b[38;2;174;0;0m:\u001b[0m\u001b[38;2;254;0;0m\n",
      "\u001b[0m\u001b[38;2;101;0;0mdef\u001b[0m\u001b[38;2;68;0;0m▁sum\u001b[0m\u001b[38;2;229;0;0mElements\u001b[0m\u001b[38;2;250;0;0mOf\u001b[0m\u001b[38;2;0;0;0mList\u001b[0m\u001b[38;2;0;0;0m(\u001b[0m\u001b[38;2;133;0;0min\u001b[0m\u001b[38;2;149;0;0m_\u001b[0m\u001b[38;2;0;0;0mlist\u001b[0m\u001b[38;2;0;0;0m):\u001b[0m\u001b[38;2;0;0;0m\n",
      "\u001b[0m\u001b[38;2;0;0;0m    \u001b[0m\u001b[38;2;61;0;0msum\u001b[0m\u001b[38;2;0;0;0m =\u001b[0m\u001b[38;2;0;0;0m \u001b[0m\u001b[38;2;0;0;0m0\u001b[0m\u001b[38;2;0;0;0m\n",
      "\u001b[0m\u001b[38;2;0;0;0m    \u001b[0m\u001b[38;2;0;0;0mfor\u001b[0m\u001b[38;2;0;0;0m i\u001b[0m\u001b[38;2;0;0;0m in\u001b[0m\u001b[38;2;235;0;0m▁list\u001b[0m\u001b[38;2;116;0;0m:\u001b[0m\u001b[38;2;0;0;0m\n",
      "\u001b[0m\u001b[38;2;0;0;0m        \u001b[0m\u001b[38;2;0;0;0msum\u001b[0m\u001b[38;2;224;0;0m▁-=\u001b[0m\u001b[38;2;204;0;0m▁i\u001b[0m\u001b[38;2;0;0;0m\n",
      "\u001b[0m\u001b[38;2;0;0;0m    \u001b[0m\u001b[38;2;0;0;0mreturn\u001b[0m\u001b[38;2;0;0;0m sum\u001b[0m\u001b[38;2;198;0;0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are debugging some code. Can you help me identify the issues in this piece of code? Can you give me for every line a probability if this line contains a bug:\n",
    "def sumElementsOfList(in_list):\n",
    "    sum = 0\n",
    "    for i in list:\n",
    "        sum -= i\n",
    "    return sum\n",
    "\"\"\"\n",
    "#with open(\"convex_hull.py\") as f:\n",
    "#    prompt = f.read()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#print(\"num_tokens\", inputs[\"input_ids\"].shape)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[0]\n",
    "\n",
    "differences = []\n",
    "\n",
    "prompt_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "result = []\n",
    "last_dif = 0\n",
    "for j, i in enumerate(logits):\n",
    "    probabilities = torch.softmax(i, 0)\n",
    "    actual_token = torch.zeros([1], dtype=torch.int32)\n",
    "    if j + 1 < len(inputs[\"input_ids\"][0]):\n",
    "        actual_token = inputs[\"input_ids\"][0][j+1]\n",
    "    act_prob = probabilities[actual_token]\n",
    "    pred_prob = torch.max(probabilities)\n",
    "\n",
    "    difference =  pred_prob.item() - act_prob.item()\n",
    "    differences.append(difference)\n",
    "\n",
    "    token = prompt_tokens[j]\n",
    "    \n",
    "    #prompt_tokens[j] = \n",
    "    #.replace(\"▁\", \" \")\n",
    "    threshold = 0.2\n",
    "    if last_dif <= threshold:\n",
    "        token = token.replace(\"▁\", \" \")\n",
    "    result.append(escape_from_hex(round(last_dif * 255), 0, 0, token))\n",
    "    last_dif = difference\n",
    "    if difference > threshold:\n",
    "        pass\n",
    "        #result.append(escape_from_hex(0, 255, 0, tokenizer.convert_ids_to_tokens([torch.argmax(probabilities)])[0]))\n",
    "        #print(f\"{difference}, {tokenizer.convert_ids_to_tokens([actual_token])}, {tokenizer.convert_ids_to_tokens([torch.argmax(probabilities)])}\")\n",
    "\n",
    "del inputs\n",
    "del logits\n",
    "del difference\n",
    "torch.cuda.empty_cache()\n",
    "# Print the prompt with colored tokens\n",
    "colored_prompt = ''.join(result)\n",
    "print(colored_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
